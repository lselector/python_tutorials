{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed4bde0",
   "metadata": {},
   "source": [
    "## PySpark Tutorial\n",
    "\n",
    "```\n",
    "Important:\n",
    "  Use bash when working with conda environments\n",
    "\n",
    "Some conda commands:\n",
    "conda info # show channels\n",
    "conda config --add channels conda-forge\n",
    "conda install --channel \"conda-forge\" pyspark\n",
    "# or conda install -c conda-forge pyspark\n",
    "\n",
    "# Add this line to your .bashrc:\n",
    "export PYSPARK_PYTHON=\"$HOME/anaconda3/bin/python\"\n",
    "\n",
    "---------------------------------------------------\n",
    "Note - Spark needs Java.\n",
    "Make sure to install it.\n",
    "Check it location in system preferences.\n",
    "Add env. variables in your .bashrc file:\n",
    "\n",
    "export JAVA_HOME='/Library/Internet Plug-Ins/JavaAppletPlugin.plugin/Contents/Home'\n",
    "export JRE_HOME=$JAVA_HOME\n",
    "\n",
    "---------------------------------------------------\n",
    "#   # Note - you can create separate environment\n",
    "#   # with specific python version if needed\n",
    "#   conda create -n py37 python=3.7\n",
    "#   conda activate py37\n",
    "#   conda install jupyter ipython numpy pandas matplotlib\n",
    "#\n",
    "#   conda env list\n",
    "#   conda activate py37\n",
    "#   conda install -c conda-forge pyspark\n",
    "```\n",
    "\n",
    "Installing PySpark on Windows:\n",
    " - https://bigdata-madesimple.com/guide-to-install-spark-and-use-pyspark-from-jupyter-in-windows/\n",
    " - https://github.com/cdarlint/winutils\n",
    "\n",
    "Note: PySpark and winutils versions should match each other.\n",
    "<br>For example, Apache PySpark 3.1.2 plus 3.2.0 winutils \n",
    "\n",
    "Spark Programming Guide\n",
    "- https://spark.apache.org/docs/latest/\n",
    "- https://spark.apache.org/docs/latest/quick-start.html\n",
    "- https://spark.apache.org/docs/latest/sql-programming-guide.html \n",
    "- https://spark.apache.org/docs/latest/rdd-programming-guide.html \n",
    "- https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\n",
    "- https://spark.apache.org/docs/latest/streaming-programming-guide.html \n",
    "- https://spark.apache.org/docs/latest/ml-guide.html \n",
    "- https://spark.apache.org/docs/latest/graphx-programming-guide.html\n",
    "- https://spark.apache.org/docs/latest/api/python/getting_started/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6053fcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28a89761",
   "metadata": {},
   "source": [
    "There are three sets of data APIs:\n",
    "- RDDs\n",
    "- DataFrames\n",
    "- Datasets"
   ]
  },
  {
   "cell_type": "raw",
   "id": "464b7cdd",
   "metadata": {},
   "source": [
    "Resilient Distributed Datasets (RDDs)\n",
    "Spark revolves around the concept of an \n",
    "RDD = Resilient Distributed Dataset\n",
    "which is a fault-tolerant collection of elements\n",
    "that can be operated on in parallel. \n",
    "\n",
    "There are two ways to create RDDs: \n",
    "- parallelizing an existing collection\n",
    "- referencing a dataset in an external storage system\n",
    "  (shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat)\n",
    "\n",
    "data = [1, 2, 3, 4, 5]\n",
    "myRDD = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7037b841",
   "metadata": {},
   "source": [
    "Spark DataFrame:\n",
    "- Like an RDD, a DataFrame is an immutable distributed collection of data. \n",
    "- Unlike an RDD, data is organized into named columns (like table in relational Database)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8e6c6a4",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "- in Java and Scala\n",
    "- a collection of strongly-typed JVM objects\n",
    "- a strongly-typed API and an untyped API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5669f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b9317bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster('local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba7fe29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/09 20:38:54 WARN Utils: Your hostname, MacBook-M1-Pro-16-Lev-2022.local resolves to a loopback address: 127.0.0.1; using 192.168.0.34 instead (on interface en0)\n",
      "22/07/09 20:38:54 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/09 20:38:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da4baf93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.34:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=pyspark-shell>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at context, \n",
    "# click on link to the Spark UI, \n",
    "# click on tabs on top\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c636e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(10**6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "977a6f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e79b1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "del sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880d610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from operator import add\n",
    "\n",
    "sc = SparkContext(master = 'local[2]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da1f6f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.0\n",
      "3.9\n",
      "local[2]\n",
      "None\n",
      "levselector\n",
      "pyspark-shell\n",
      "local-1657413539953\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Inspect SparkContext\n",
    "print( sc.version )              # SparkContext version \n",
    "print( sc.pythonVer )            # Python version \n",
    "print( sc.master )               # Master URL to connect to \n",
    "print( str(sc.sparkHome) )       # Path where Spark is installed on worker nodes \n",
    "print( str(sc.sparkUser()))      # name of the Spark User running SparkContext\n",
    "print( sc.appName )              # application name \n",
    "print( sc.applicationId )        # application ID \n",
    "print( sc.defaultParallelism )   # default level of parallelism \n",
    "print( sc.defaultMinPartitions ) # minimum number of partitions for RDDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5521480",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "del sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ef3c713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local\")\n",
    "         .setAppName(\"My app\")\n",
    "         .set(\"spark.executor.memory\", \"1g\")) \n",
    "\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "95982372",
   "metadata": {},
   "source": [
    "Using The Shell\n",
    "In the PySpark shell, a special interpreter-aware SparkContext \n",
    "is already created in the variable called sc.\n",
    "\n",
    "$ ./bin/spark-shell --master local[2] \n",
    "$ ./bin/pyspark --master local[4] --py-files code.py\n",
    "\n",
    "Set which master the context connects to with the --master argument, \n",
    "and add Python .zip, .egg or .py files to the runtime path \n",
    "by passing a comma-separated list to --py-files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af8adf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "\n",
    "# Parallelized Collections\n",
    "\n",
    "rdd  = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd2 = sc.parallelize([('a',2),('d',1),('b',1)])\n",
    "rdd3 = sc.parallelize(range(100))\n",
    "rdd4 = sc.parallelize([(\"a\",[\"x\",\"y\",\"z\"]), (\"b\",[\"p\", \"r\"])])\n",
    "\n",
    "# External Data\n",
    "# Read either one text file from HDFS, \n",
    "# a local file system \n",
    "# or any Hadoop-supported file system URI with textFile(), \n",
    "# or read in a directory of text files with wholeTextFiles().\n",
    "\n",
    "textFile = sc.textFile(\"/tmp/lev/*.txt\")\n",
    "textFile2 = sc.wholeTextFiles(\"/tmp/lev/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b8ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "defaultdict(<class 'int'>, {'a': 2, 'b': 1})\n",
      "defaultdict(<class 'int'>, {('a', 7): 1, ('a', 2): 1, ('b', 2): 1})\n",
      "{'a': 2, 'b': 2}\n",
      "4950\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Retrieving RDD Information\n",
    "# Basic Information\n",
    "\n",
    "print( rdd.getNumPartitions() ) # number of partitions \n",
    "  # 1\n",
    "print( rdd.count() )            # Count RDD instances    \n",
    "  # 3\n",
    "print( rdd.countByKey() )       # Count RDD instances by key \n",
    "  # defaultdict(<class 'int'>, {'a': 2, 'b': 1})\n",
    "    \n",
    "print(rdd.countByValue())       # Count RDD instances by value \n",
    "  # defaultdict(<class 'int'>, {('a', 7): 1, ('a', 2): 1, ('b', 2): 1})\n",
    "\n",
    "print( rdd.collectAsMap() )     # Return (key,value) pairs as a dictionary\n",
    "  # {'a': 2,'b': 2}\n",
    "    \n",
    "print( rdd3.sum() )             # Sum of RDD elements \n",
    "  # 4950\n",
    "\n",
    "print( sc.parallelize([]).isEmpty() ) # Check whether RDD is empty\n",
    "  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaa52ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "0\n",
      "49.5\n",
      "28.86607004772212\n",
      "833.25\n",
      "([0, 33, 66, 99], [33, 33, 34])\n",
      "(count: 100, mean: 49.5, stdev: 28.86607004772212, max: 99.0, min: 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "\n",
    "print( rdd3.max() )   # Maximum value of RDD elements\n",
    "  # 99\n",
    "print( rdd3.min() )   # Minimum value of RDD elements\n",
    "  # 0\n",
    "print( rdd3.mean() )  # Mean value of RDD elements\n",
    "  # 49.5\n",
    "print( rdd3.stdev() ) # Standard deviation of RDD elements\n",
    "  # 28.866\n",
    "print( rdd3.variance() )   # variance of RDD elements \n",
    "  # 833.25\n",
    "print( rdd3.histogram(3) ) # histogram by bins  \n",
    "  # ([0,33,66,99],[33,33,34])\n",
    "print( rdd3.stats() )      # Summary statistics \n",
    "                           # (count, mean, stdev, max & min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac69627b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7, 7, 'a'), ('a', 2, 2, 'a'), ('b', 2, 2, 'b')]\n",
      "['a', 7, 7, 'a', 'a', 2, 2, 'a', 'b', 2, 2, 'b']\n",
      "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n"
     ]
    }
   ],
   "source": [
    "# Applying Functions\n",
    "\n",
    "# Apply a function to each RDD element\n",
    "print( rdd.map(lambda x: x+(x[1],x[0])).collect() )\n",
    "  # [('a',7,7,'a'),('a',2,2,'a'),('b',2,2,'b')] \n",
    "    \n",
    "# Apply a function to each RDD element and flatten the result\n",
    "rdd5 = rdd.flatMap(lambda x: x+(x[1],x[0]))\n",
    "print( rdd5.collect() )\n",
    "  # ['a',7,7,'a','a',2,2,'a','b',2,2,'b'] \n",
    "\n",
    "# Apply a flatMap function to each (key,value) \n",
    "# pair of rdd4 without changing the keys\n",
    "print( rdd4.flatMapValues(lambda x: x).collect() )\n",
    "  # [('a','x'),('a','y'),('a','z'),('b','p'),('b','r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c436e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('a', 2), ('b', 2)]\n",
      "[('a', 7), ('a', 2)]\n",
      "('a', 7)\n",
      "[('b', 2), ('a', 7)]\n"
     ]
    }
   ],
   "source": [
    "# Selecting Data\n",
    "# -------------------------------\n",
    "# Getting\n",
    "\n",
    "# Return a list with all RDD elements \n",
    "print( rdd.collect() )\n",
    "    # [('a', 7), ('a', 2), ('b', 2)] \n",
    "\n",
    "# Take first 2 RDD elements \n",
    "print( rdd.take(2) )\n",
    "    # [('a', 7), ('a', 2)] \n",
    "\n",
    "# Take first RDD element \n",
    "print( rdd.first() )\n",
    "    # ('a', 7) \n",
    "\n",
    "# Take top 2 RDD elements\n",
    "print( rdd.top(2) )\n",
    "    # [('b', 2), ('a', 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b646c889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 27, 28, 35, 41, 43, 49, 53, 58, 85, 93]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Sampling\n",
    "# Return sampled subset of rdd3\n",
    "print( rdd3.sample(False, 0.15, 81).collect() )\n",
    "    # [3,4,27,31,40,41,42,43,60,76,79,80,86,97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdbc92f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('a', 2)]\n",
      "['a', 7, 2, 'b']\n",
      "['a', 'a', 'b']\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Filtering \n",
    "# Filter the RDD\n",
    "print( rdd.filter(lambda x: \"a\" in x).collect() )\n",
    "    # [('a',7),('a',2)] \n",
    "    \n",
    "# Return distinct RDD values \n",
    "print( rdd5.distinct().collect() )\n",
    "    # ['a',2,'b',7] \n",
    "\n",
    "# Return (key,value) RDD's keys\n",
    "print( rdd.keys().collect() )\n",
    "    # ['a', 'a', 'b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73220dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "('a', 7)\n",
      "('a', 2)\n",
      "('b', 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterating\n",
    "# Apply a function to all RDD elements\n",
    "def g(x): \n",
    "    print(x)\n",
    "print( rdd.foreach(g) )\n",
    "  # None\n",
    "    \n",
    "rdd.take(3)\n",
    "  # ('a', 7) ('b', 2) ('a', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f8d7162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 9), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Reshaping Data\n",
    "\n",
    "# -------------------------------\n",
    "# Reducing\n",
    "# Merge the rdd values for each key\n",
    "print( rdd.reduceByKey(lambda x,y : x+y).collect() )\n",
    "   # [('a',9),('b',2)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5976e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('a', 7, 'a', 2, 'b', 2)\n"
     ]
    }
   ],
   "source": [
    "# Merge the rdd values\n",
    "print( rdd.reduce(lambda a, b: a + b) )\n",
    "   # ('a',7,'a',2,'b',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba0cf28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, [0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98]), (1, [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99])]\n",
      "[('a', [7, 2]), ('b', [2])]\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Grouping by\n",
    "print( rdd3.groupBy(lambda x: x % 2)\n",
    "      .mapValues(list)\n",
    "      .collect() )  # Return RDD of grouped values\n",
    "print( rdd.groupByKey()\n",
    "      .mapValues(list)\n",
    "      .collect() )  # Group rdd by key\n",
    "   # [('a',[7,2]),('b',[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b889ad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Aggregating\n",
    "\n",
    "seqOp = (lambda x,y: (x[0]+y,x[1]+1)) \n",
    "combOp = (lambda x,y:(x[0]+y[0],x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c56eaacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4950, 100)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate RDD elements of each partition and then the results \n",
    "rdd3.aggregate((0,0),seqOp,combOp)\n",
    "    # (4950,100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9fb15ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (9, 2)), ('b', (2, 1))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate values of each RDD key\n",
    "rdd.aggregateByKey((0,0),seqOp,combOp).collect() \n",
    "    # [('a',(9,2)), ('b',(2,1))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97de102c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4950"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate the elements of each partition, and then the results\n",
    "rdd3.fold(0,add)\n",
    "    # 4950 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fca0c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (2, 1),\n",
       " (4, 2),\n",
       " (6, 3),\n",
       " (8, 4),\n",
       " (10, 5),\n",
       " (12, 6),\n",
       " (14, 7),\n",
       " (16, 8),\n",
       " (18, 9),\n",
       " (20, 10),\n",
       " (22, 11),\n",
       " (24, 12),\n",
       " (26, 13),\n",
       " (28, 14),\n",
       " (30, 15),\n",
       " (32, 16),\n",
       " (34, 17),\n",
       " (36, 18),\n",
       " (38, 19),\n",
       " (40, 20),\n",
       " (42, 21),\n",
       " (44, 22),\n",
       " (46, 23),\n",
       " (48, 24),\n",
       " (50, 25),\n",
       " (52, 26),\n",
       " (54, 27),\n",
       " (56, 28),\n",
       " (58, 29),\n",
       " (60, 30),\n",
       " (62, 31),\n",
       " (64, 32),\n",
       " (66, 33),\n",
       " (68, 34),\n",
       " (70, 35),\n",
       " (72, 36),\n",
       " (74, 37),\n",
       " (76, 38),\n",
       " (78, 39),\n",
       " (80, 40),\n",
       " (82, 41),\n",
       " (84, 42),\n",
       " (86, 43),\n",
       " (88, 44),\n",
       " (90, 45),\n",
       " (92, 46),\n",
       " (94, 47),\n",
       " (96, 48),\n",
       " (98, 49),\n",
       " (100, 50),\n",
       " (102, 51),\n",
       " (104, 52),\n",
       " (106, 53),\n",
       " (108, 54),\n",
       " (110, 55),\n",
       " (112, 56),\n",
       " (114, 57),\n",
       " (116, 58),\n",
       " (118, 59),\n",
       " (120, 60),\n",
       " (122, 61),\n",
       " (124, 62),\n",
       " (126, 63),\n",
       " (128, 64),\n",
       " (130, 65),\n",
       " (132, 66),\n",
       " (134, 67),\n",
       " (136, 68),\n",
       " (138, 69),\n",
       " (140, 70),\n",
       " (142, 71),\n",
       " (144, 72),\n",
       " (146, 73),\n",
       " (148, 74),\n",
       " (150, 75),\n",
       " (152, 76),\n",
       " (154, 77),\n",
       " (156, 78),\n",
       " (158, 79),\n",
       " (160, 80),\n",
       " (162, 81),\n",
       " (164, 82),\n",
       " (166, 83),\n",
       " (168, 84),\n",
       " (170, 85),\n",
       " (172, 86),\n",
       " (174, 87),\n",
       " (176, 88),\n",
       " (178, 89),\n",
       " (180, 90),\n",
       " (182, 91),\n",
       " (184, 92),\n",
       " (186, 93),\n",
       " (188, 94),\n",
       " (190, 95),\n",
       " (192, 96),\n",
       " (194, 97),\n",
       " (196, 98),\n",
       " (198, 99)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the values for each key\n",
    "rdd.foldByKey(0, add).collect() \n",
    "    # [('a',9),('b',2)] \n",
    "\n",
    "# Create tuples of RDD elements by applying a function\n",
    "rdd3.keyBy(lambda x: x+x).collect()\n",
    "    # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9d90609b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 7), ('b', 2)]\n",
      "[('d', 1)]\n",
      "[(('a', 7), ('a', 2)), (('a', 7), ('d', 1)), (('a', 7), ('b', 1)), (('a', 2), ('a', 2)), (('a', 2), ('d', 1)), (('a', 2), ('b', 1)), (('b', 2), ('a', 2)), (('b', 2), ('d', 1)), (('b', 2), ('b', 1))]\n"
     ]
    }
   ],
   "source": [
    "# Mathematical Operations\n",
    "\n",
    "# Return each rdd value not contained in rdd2\n",
    "print( rdd.subtract(rdd2).collect() )\n",
    "  # [('b',2),('a',7)] \n",
    "    \n",
    "# Return each (key,value) pair of rdd2 with no matching key in rdd\n",
    "print( rdd2.subtractByKey(rdd).collect() )\n",
    "  # [('d', 1)] \n",
    "\n",
    "# Return the Cartesian product of rdd and rdd2\n",
    "print( rdd.cartesian(rdd2).collect() )\n",
    "  # [(('a', 7), ('a', 2)), \n",
    "  #  (('a', 7), ('d', 1)), \n",
    "  #  (('a', 7), ('b', 1)), \n",
    "  #  (('a', 2), ('a', 2)), \n",
    "  #  (('a', 2), ('d', 1)), \n",
    "  #  (('a', 2), ('b', 1)), \n",
    "  #  (('b', 2), ('a', 2)), \n",
    "  #  (('b', 2), ('d', 1)), \n",
    "  #  (('b', 2), ('b', 1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a0f1578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d', 1), ('b', 1), ('a', 2)]\n",
      "[('a', 2), ('b', 1), ('d', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Sort\n",
    "\n",
    "# Sort RDD by given function\n",
    "print( rdd2.sortBy(lambda x: x[1]).collect() )\n",
    "  # [('d',1),('b',1),('a',2)] \n",
    "\n",
    "# Sort (key, value) RDD by key\n",
    "print( rdd2.sortByKey().collect() )\n",
    "  # [('a',2),('b',1),('d',1)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea3e83a9",
   "metadata": {},
   "source": [
    "# Repartitioning\n",
    "rdd.repartition(4) # New RDD with 4 partitions\n",
    "rdd.coalesce(1)    # Decrease the number of partitions in the RDD to 1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dcefdddf",
   "metadata": {},
   "source": [
    "# Saving\n",
    "rdd.saveAsTextFile(\"rdd.txt\")\n",
    "rdd.saveAsHadoopFile(\n",
    "    \"hdfs://namenodehost/parent/child\", \n",
    "    \"org.apache.hadoop.mapred.TextOutputFormat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d5e9c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping SparkContext\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfee8889",
   "metadata": {},
   "source": [
    "# Execution\n",
    "$ ./bin/spark-submit examples/src/main/python/pi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54758de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
